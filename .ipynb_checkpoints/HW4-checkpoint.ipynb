{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.image as mpimg\n",
    "import warnings\n",
    "import time\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Download CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 dataset was downloaded from the following URL:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to unpickle the file\n",
    "#Adapted from the original given at the following URL\n",
    "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def unpickle(file):\n",
    "    fo=open(file,'rb')\n",
    "    dict=pickle.load(fo,encoding = 'latin1')\n",
    "    fo.close()\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  batch_1=unpickle('data_batch_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks expect feature scaling for input features or else if we use Gradient Descent the weights update step is affected based on the value of the input and hence can lead to unwanted results. The following function therefore returns normalized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to convert data into training, validation and testing\n",
    "def get_datasets():\n",
    "    \"\"\"\n",
    "    Description : Returns normalized training, testing and validation datasets along with labels\n",
    "    Returns : A list containing following tuples\n",
    "          training_data,training_labels : A tuple with data and labels for training\n",
    "          validation_data, validation_labels : A tuple with data and labels for validation\n",
    "          testing_data : A tuple with data and labels for testing data\n",
    "          label_names : A list containing label names\n",
    "          minmaxscale : Feature Scaling object\n",
    "    \"\"\"\n",
    "\n",
    "    batch_1=unpickle('data_batch_1')\n",
    "    batch_2=unpickle('data_batch_2')\n",
    "    batch_3=unpickle('data_batch_3')\n",
    "    batch_4=unpickle('data_batch_4')\n",
    "    validation_batch=unpickle('data_batch_5')\n",
    "    test_batch = unpickle('test_batch')\n",
    "     \n",
    "    train_dataset_1 = batch_1['data']\n",
    "    train_dataset_2 = batch_2['data']\n",
    "    train_dataset_3 = batch_3['data']\n",
    "    train_dataset_4 = batch_4['data']\n",
    "   \n",
    "    \n",
    "    training_data = np.vstack((train_dataset_1,train_dataset_2,train_dataset_3,train_dataset_4))\n",
    "    #Normalize the training data\n",
    "    normalizer = MinMaxScaler().fit(training_data*1.)\n",
    "    \n",
    "    training_data = np.float32(normalizer.transform(training_data))\n",
    "    #Combine individual training datasets\n",
    "    training_data_labels = np.hstack((batch_1['labels'],batch_2['labels'],batch_3['labels'],batch_4['labels']))\n",
    "    \n",
    "    validation_data = validation_batch['data']\n",
    "    validation_data = np.float32(normalizer.transform(validation_data*1.))\n",
    "    validation_data_labels = np.asarray(validation_batch['labels'])\n",
    "\n",
    "    test_data = test_batch['data']\n",
    "    test_data = np.float32(normalizer.transform(test_data*1.))\n",
    "    test_data_labels = np.asarray(test_batch['labels'])\n",
    "    \n",
    "    label_names = unpickle('batches.meta')\n",
    "    \n",
    "    return [(training_data,training_data_labels),(validation_data,validation_data_labels),(test_data,test_data_labels),label_names,normalizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABlCAYAAABp/WZxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbee0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot One image each from training, testing and validation datasets\n",
    "datasets = get_datasets()\n",
    "training_tuple = datasets[0]\n",
    "validation_tuple = datasets[1]\n",
    "testing_tuple = datasets[2]\n",
    "label_names = datasets[3]\n",
    "index = 10\n",
    "#Get RGB data as numpy arrays\n",
    "training_images = training_tuple[0].reshape((training_tuple[0].shape[0],32,32,3),order='f')\n",
    "test_images = testing_tuple[0].reshape((testing_tuple[0].shape[0],32,32,3),order='f')\n",
    "validation_images = validation_tuple[0].reshape((validation_tuple[0].shape[0],32,32,3),order='f')\n",
    "\n",
    "f, axs = plt.subplots(1, 9, sharex=True,figsize=(10,1))\n",
    "\n",
    "for i in range(7):\n",
    "    index = index +i\n",
    "    training_image=training_images[index,:,:,:]\n",
    "    validation_image = validation_images[index,:,:,:]\n",
    "    test_image = test_images[index,:,:,:]\n",
    "    axs[i].imshow(np.rot90(training_image,3))\n",
    "    axs[i].set_title(label_names['label_names'][training_tuple[1][index]])\n",
    "    axs[i].axis('off')\n",
    "    axs[i+1].imshow(np.rot90(validation_image,3))\n",
    "    axs[i+1].set_title(label_names['label_names'][validation_tuple[1][index]])\n",
    "    axs[i+1].axis('off')\n",
    "    axs[i+2].imshow(np.rot90(test_image,3))\n",
    "    axs[i+2].set_title(label_names['label_names'][testing_tuple[1][index]])\n",
    "    axs[i+2].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of labels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n",
      "Class: truck\n",
      "One hot encoded representation: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding of Labels\n",
    "training_labels = training_tuple[1]\n",
    "print('Label:',training_labels[15])\n",
    "print('Class:',label_names['label_names'][9])\n",
    "training_labels_enc = (np.arange(10) == training_labels[:,None]).astype(np.float32)\n",
    "training_labels_encf = training_labels_enc\n",
    "training_images_f=training_images\n",
    "print('One hot encoded representation:',training_labels_enc[15])\n",
    "validation_labels = validation_tuple[1]\n",
    "validation_labels_enc = (np.arange(10) == validation_labels[:,None]).astype(np.float32)\n",
    "testing_labels = testing_tuple[1]\n",
    "testing_labels_enc = (np.arange(10) == testing_labels[:,None]).astype(np.float32)\n",
    "testing_labels_encf = testing_labels_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The example output given above verifies that one hot encoding is correct._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Select a Deep Learning Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use TensorFlow for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Establish Baseline Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Train and Test a Generic Deep Neural Network and record the training time and the accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a model with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is taken from Section 7 notebook\n",
    "#Some utility functions to be used for computation graph building and execution\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "split_by_half = lambda x,k : int(x/2**k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3d01e1d4ad79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Training computation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mregularizers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1_biases\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2_biases\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer3_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer3_biases\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[0;32m   1742\u001b[0m   \"\"\"\n\u001b[0;32m   1743\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[1;32m-> 1744\u001b[1;33m                     labels, logits)\n\u001b[0m\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m   \u001b[1;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[1;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[0;32m   1696\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[1;32m-> 1698\u001b[1;33m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[0;32m   1699\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "#Save training, testing and validation datasets \n",
    "#in corresponding variables\n",
    "training_data = training_tuple[0]\n",
    "testing_data = testing_tuple[0]\n",
    "validation_data = validation_tuple[0]\n",
    "#Constants\n",
    "hidden_nodes = 2000\n",
    "batch_size = 256\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "#Dictionary to save prediction results\n",
    "test_preds={}\n",
    "#Regularization penalty\n",
    "regularization_constant = 0.01\n",
    "#The following code is adapted from Section 7 notebook\n",
    "#This portion will construct the computation Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_of_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    \n",
    "    #Constants\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([num_of_pixels, hidden_nodes])\n",
    "    layer1_biases = bias_variable([hidden_nodes])\n",
    "    layer2_weights = weight_variable([hidden_nodes, split_by_half(hidden_nodes,1)])\n",
    "    layer2_biases = bias_variable([split_by_half(hidden_nodes,1)])   \n",
    "    layer3_weights = weight_variable([split_by_half(hidden_nodes,1), 10])\n",
    "    layer3_biases = bias_variable([10])\n",
    "    \n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    \n",
    "    # Model with dropout\n",
    "    def model(data):\n",
    "        layer1 = tf.matmul(data, layer1_weights) + layer1_biases\n",
    "        hidden1 = tf.nn.relu(layer1)  \n",
    "        layer2 = tf.matmul(hidden1, layer2_weights) + layer2_biases  # a new hidden layer\n",
    "        hidden2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        return tf.matmul(hidden2, layer3_weights) + layer3_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \\\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + \\\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    loss = tf.reduce_mean(loss + regularization_constant * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    # learning rate decay\n",
    "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from Section 7 notebook\n",
    "def run_session(num_epochs, name):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3Logs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch accuracy: {:.1f} %\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f} %\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f} %\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "\n",
    "run_session(2000, \"Q3A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the model using Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we visualized the model by using Tensorboard.\n",
    "\n",
    "**python /Applications/anaconda/envs/tensorflow/bin/tensorboard --logdir=/tmp/HW4Q3Logs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph showed different pieces of the model and allowed us to see the values passed on to different stage. We think that Tensorboard can be invaluable tool, once we become more well versed in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Train and test a generic convolutional network on the data. Record the training time (your choice of approaches) and the accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code is taken from Section 7\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-define the function to include the keep probability\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3blogs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_data = batch_data.reshape(-1,32,32,3)\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_session(2000,\"CNN\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Train and test a non-NN algorithm with reasonable performance on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected Logistic Regression with Stochastic Gradient Descent as our non NN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "batch_size = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data, with a placeholder to feed the training data at run time.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 3072))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([3072, 10]))\n",
    "    biases = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3001\n",
    "l_array = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    merged = tf.merge_all_summaries()  \n",
    "    writer = tf.train.SummaryWriter(\"/tmp/HW4Q3cLogs\", session.graph)\n",
    "    print(\"Initialized\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = training_data[offset:(offset + batch_size), :]\n",
    "        batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        l_array.append(l)\n",
    "        if (epoch % 500 == 0):\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), validation_labels_enc))\n",
    "    model_time = time.time() - start_time\n",
    "    print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), testing_labels_enc))\n",
    "    test_preds['SGD'] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d) Compare the three methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Training Times**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compare Training Time\n",
    "methods = ['NN','CNN','SGD']\n",
    "training_times = [1738,182.4,8.8]\n",
    "plt.bar(np.arange(len(training_times)),training_times, align='center', alpha=0.5, color={'b','g','r'})\n",
    "plt.xticks(np.arange(len(training_times)), methods)\n",
    "plt.yticks( training_times)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Training Time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compare Accuracy\n",
    "methods = ['NN','CNN','SGD']\n",
    "accuracy = [42.9,35.2,29.2]\n",
    "plt.bar(np.arange(len(accuracy)),accuracy, align='center', alpha=0.5, color={'c','m','y'})\n",
    "plt.xticks(np.arange(len(accuracy)), methods)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots given above, we can see that Neural Network with 2 hidden layers took the most time to train. The reason it took more time than Convolutional Neural Network is because it was a fully connected network and we have not used dropouts. Convolutional Neural Network on the other hand took half as much time as MLP based NN as the algorithm took advantage of reduced number of weights calculations in backpropagation step. SGD was quick but the accuracy was low compared to the Neural Network based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Assess the sensitivity of the test set to training data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Randomly divide the training data into at least 3 pieces (like cross-validation): A, B, C, ... but do so in a way that preserves the an equal fraction of the 10 categories of images across sets. That is, each set {A,B, etc.} should have the same number of category 1, 2,...10 images within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = training_tuple[1].reshape(training_tuple[1].shape[0],1)\n",
    "combined = np.hstack((training_tuple[0],labels))\n",
    "df_training = pd.DataFrame(combined)\n",
    "df_training[3072].value_counts()\n",
    "classes= {}\n",
    "for index in range(10):\n",
    "    classes[index] = (pd.DataFrame(df_training.loc[df_training[3072] == index])).iloc[0:3900,:]\n",
    "\n",
    "x_A=np.vstack((classes[0].iloc[0:1300],classes[1].iloc[0:1300],classes[2].iloc[0:1300],classes[3].iloc[0:1300],classes[4].iloc[0:1300],classes[5].iloc[0:1300],classes[6].iloc[0:1300],classes[7].iloc[0:1300],classes[8].iloc[0:1300],classes[9].iloc[0:1300]))\n",
    "x_B=np.vstack((classes[0].iloc[1300:2600],classes[1].iloc[1300:2600],classes[2].iloc[1300:2600],classes[3].iloc[1300:2600],classes[4].iloc[1300:2600],classes[5].iloc[1300:2600],classes[6].iloc[1300:2600],classes[7].iloc[1300:2600],classes[8].iloc[1300:2600],classes[9].iloc[1300:2600]))\n",
    "x_C=np.vstack((classes[0].iloc[2600:3900],classes[1].iloc[2600:3900],classes[2].iloc[2600:3900],classes[3].iloc[2600:3900],classes[4].iloc[2600:3900],classes[5].iloc[2600:3900],classes[6].iloc[2600:3900],classes[7].iloc[2600:3900],classes[8].iloc[2600:3900],classes[9].iloc[2600:3900]))\n",
    "np.random.shuffle(x_A)\n",
    "np.random.shuffle(x_B)\n",
    "np.random.shuffle(x_C)\n",
    "x_A_B=np.vstack((x_A,x_B))\n",
    "df_A = pd.DataFrame(x_A)\n",
    "df_B = pd.DataFrame(x_B)\n",
    "df_C = pd.DataFrame(x_C)\n",
    "df_A_B =pd.DataFrame(x_A_B)\n",
    "\n",
    "df_label_A = df_A[df_A.columns[[3072]]]\n",
    "df_label_B = df_B[df_B.columns[[3072]]]\n",
    "df_label_C = df_C[df_C.columns[[3072]]]\n",
    "df_label_A_B=df_A_B[df_A_B.columns[[3072]]]\n",
    "training_A=df_A.as_matrix\n",
    "\n",
    "df_A.drop(df_A.columns[[3072]],axis=1,inplace=True)\n",
    "df_B.drop(df_B.columns[[3072]],axis=1,inplace=True)\n",
    "df_C.drop(df_C.columns[[3072]],axis=1,inplace=True)\n",
    "df_A_B.drop(df_A_B.columns[[3072]],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "training_A=df_A.as_matrix()\n",
    "label_A=df_label_A.as_matrix()[:,0]\n",
    "training_B=df_B.as_matrix()\n",
    "label_B=df_label_B.as_matrix()[:,0]\n",
    "training_A_B=df_A_B.as_matrix()\n",
    "label_A_B=df_label_A_B.as_matrix()[:,0]\n",
    "label_A_B.shape\n",
    "training_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Train your classifiers in part 3 on each of the following, A, AB, etc. and test on the full test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using the first dataset (i.e A) on Q3 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One hot encoding of Labels\n",
    "#training_labels = training_tuple[1]\n",
    "training_labels = label_A\n",
    "training_labels_enc = (np.arange(10) == training_labels[:,None]).astype(np.float32)\n",
    "validation_labels = validation_tuple[1]\n",
    "validation_labels_enc = (np.arange(10) == validation_labels[:,None]).astype(np.float32)\n",
    "testing_labels = testing_tuple[1]\n",
    "testing_labels_enc = (np.arange(10) == testing_labels[:,None]).astype(np.float32)\n",
    "#The following code is taken from Section 7 notebook\n",
    "#Some utility functions to be used for computation graph building and execution\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "split_by_half = lambda x,k : int(x/2**k)\n",
    "#Save training, testing and validation datasets \n",
    "#in corresponding variables\n",
    "#training_data = training_tuple[0]\n",
    "training_data = training_A\n",
    "testing_data = testing_tuple[0]\n",
    "validation_data = validation_tuple[0]\n",
    "#Constants\n",
    "hidden_nodes = 2000\n",
    "batch_size = 256\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "#Dictionary to save prediction results\n",
    "test_preds={}\n",
    "#Regularization penalty\n",
    "regularization_constant = 0.01\n",
    "#The following code is adapted from Section 7 notebook\n",
    "#This portion will construct the computation Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_of_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    \n",
    "    #Constants\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([num_of_pixels, hidden_nodes])\n",
    "    layer1_biases = bias_variable([hidden_nodes])\n",
    "    layer2_weights = weight_variable([hidden_nodes, split_by_half(hidden_nodes,1)])\n",
    "    layer2_biases = bias_variable([split_by_half(hidden_nodes,1)])   \n",
    "    layer3_weights = weight_variable([split_by_half(hidden_nodes,1), 10])\n",
    "    layer3_biases = bias_variable([10])\n",
    "    \n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    \n",
    "    # Model with dropout\n",
    "    def model(data):\n",
    "        layer1 = tf.matmul(data, layer1_weights) + layer1_biases\n",
    "        hidden1 = tf.nn.relu(layer1)  \n",
    "        layer2 = tf.matmul(hidden1, layer2_weights) + layer2_biases  # a new hidden layer\n",
    "        hidden2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        return tf.matmul(hidden2, layer3_weights) + layer3_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \\\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + \\\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    loss = tf.reduce_mean(loss + regularization_constant * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    # learning rate decay\n",
    "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    #The following code is adapted from Section 7 notebook\n",
    "def run_session(num_epochs, name):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3Logs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch accuracy: {:.1f} %\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f} %\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f} %\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "\n",
    "run_session(2000, \"Q4a/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and test a generic convolutional network using A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code is taken from Section 7\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))\n",
    "# Re-define the function to include the keep probability\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3blogs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_data = batch_data.reshape(-1,32,32,3)\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "run_session(2000,\"CNN\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and test a non NN network using A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data, with a placeholder to feed the training data at run time.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 3072))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([3072, 10]))\n",
    "    biases = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "num_epochs = 3001\n",
    "l_array = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    merged = tf.merge_all_summaries()  \n",
    "    writer = tf.train.SummaryWriter(\"/tmp/HW4Q3cLogs\", session.graph)\n",
    "    print(\"Initialized\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = training_data[offset:(offset + batch_size), :]\n",
    "        batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        l_array.append(l)\n",
    "        if (epoch % 500 == 0):\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), validation_labels_enc))\n",
    "    model_time = time.time() - start_time\n",
    "    print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), testing_labels_enc))\n",
    "    test_preds['SGD'] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using the A+B on Q3 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One hot encoding of Labels\n",
    "#training_labels = training_tuple[1]\n",
    "training_labels = label_A_B\n",
    "training_labels_enc = (np.arange(10) == training_labels[:,None]).astype(np.float32)\n",
    "validation_labels = validation_tuple[1]\n",
    "validation_labels_enc = (np.arange(10) == validation_labels[:,None]).astype(np.float32)\n",
    "testing_labels = testing_tuple[1]\n",
    "testing_labels_enc = (np.arange(10) == testing_labels[:,None]).astype(np.float32)\n",
    "#The following code is taken from Section 7 notebook\n",
    "#Some utility functions to be used for computation graph building and execution\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "split_by_half = lambda x,k : int(x/2**k)\n",
    "#Save training, testing and validation datasets \n",
    "#in corresponding variables\n",
    "#training_data = training_tuple[0]\n",
    "training_data = training_A_B\n",
    "testing_data = testing_tuple[0]\n",
    "validation_data = validation_tuple[0]\n",
    "#Constants\n",
    "hidden_nodes = 2000\n",
    "batch_size = 256\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "#Dictionary to save prediction results\n",
    "test_preds={}\n",
    "#Regularization penalty\n",
    "regularization_constant = 0.01\n",
    "#The following code is adapted from Section 7 notebook\n",
    "#This portion will construct the computation Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_of_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    \n",
    "    #Constants\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([num_of_pixels, hidden_nodes])\n",
    "    layer1_biases = bias_variable([hidden_nodes])\n",
    "    layer2_weights = weight_variable([hidden_nodes, split_by_half(hidden_nodes,1)])\n",
    "    layer2_biases = bias_variable([split_by_half(hidden_nodes,1)])   \n",
    "    layer3_weights = weight_variable([split_by_half(hidden_nodes,1), 10])\n",
    "    layer3_biases = bias_variable([10])\n",
    "    \n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    \n",
    "    # Model with dropout\n",
    "    def model(data):\n",
    "        layer1 = tf.matmul(data, layer1_weights) + layer1_biases\n",
    "        hidden1 = tf.nn.relu(layer1)  \n",
    "        layer2 = tf.matmul(hidden1, layer2_weights) + layer2_biases  # a new hidden layer\n",
    "        hidden2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        return tf.matmul(hidden2, layer3_weights) + layer3_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \\\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + \\\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    loss = tf.reduce_mean(loss + regularization_constant * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    # learning rate decay\n",
    "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    #The following code is adapted from Section 7 notebook\n",
    "def run_session(num_epochs, name):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3Logs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch accuracy: {:.1f} %\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f} %\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f} %\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "\n",
    "run_session(2000, \"Q4aAandB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and test a generic convolutional network using A+B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 4b with A: Train and test a generic convolutional network on the data. \n",
    "#This code is taken from Section 7\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))\n",
    "# Re-define the function to include the keep probability\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q3blogs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_data = batch_data.reshape(-1,32,32,3)\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "run_session(2000,\"CNN_AandB\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and test a non NN network using A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 4b  with A:  Train and test a non-NN algorithm with reasonable performance on the data.\n",
    "#The following code is adapted from the code given in Section 7\n",
    "batch_size = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data, with a placeholder to feed the training data at run time.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 3072))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(testing_tuple[0])\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([3072, 10]))\n",
    "    biases = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "num_epochs = 3001\n",
    "l_array = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    merged = tf.merge_all_summaries()  \n",
    "    writer = tf.train.SummaryWriter(\"/tmp/HW4Q3cLogs\", session.graph)\n",
    "    print(\"Initialized\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = training_data[offset:(offset + batch_size), :]\n",
    "        batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        l_array.append(l)\n",
    "        if (epoch % 500 == 0):\n",
    "            print(\"Minibatch loss at epoch %d: %f\" % (epoch, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), validation_labels_enc))\n",
    "    model_time = time.time() - start_time\n",
    "    print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), testing_labels_enc))\n",
    "    test_preds['SGD'] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c) Plot and comment on how quickly the classifiers learn as compared to 3) using the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing the training accuracy\n",
    "NN_times = (400.6,393.2,1738)\n",
    "CNN_times = (201.6,173.5,182.4)\n",
    "Non_NN_times = (13.3,10.7,8.8)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(3)\n",
    "\n",
    "#Set bar width\n",
    "width = 0.2\n",
    "NN_plot = plt.bar(index, NN_times, width,\n",
    "                 alpha=0.7,\n",
    "                 color='c',\n",
    "                 label='Deep NN')\n",
    " \n",
    "CNN_plot = plt.bar(index + width, CNN_times, width,\n",
    "                 alpha=0.7,\n",
    "                 color='g',\n",
    "                 label='CNN')\n",
    "NonNN_plot = plt.bar(index + 2*width, Non_NN_times, width,\n",
    "                 alpha=0.7,\n",
    "                 color='r',\n",
    "                 label='Non NN')\n",
    " \n",
    "plt.xlabel('Training Data')\n",
    "plt.ylabel('Training Time')\n",
    "plt.title('Training Time comparison for different datasets')\n",
    "plt.xticks(index + width, ('A only', 'A+B', 'All'))\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the change in training data (splitting in A, A+B and full) did not have huge impact on training time. However it is evident that Deep NN is taking most time for training and Non Neural network is fastest.This speed ofcourse comes at the price of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The reason Deep NN  took more time than Convolutional Neural Network is because it was a fully connected network and we have not used dropouts. Convolutional Neural Network on the other hand took half as much time as MLP based NN as the algorithm took advantage of reduced number of weights calculations in backpropagation step. SGD was quick but the accuracy was low compared to the Neural Network based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5) Assess the sensitivity of the test set to distortions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a) Create at least one additional test sets by distorting the test set in one manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check modifying saturation of a single image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(1,1))\n",
    "plt.imshow(np.rot90(training_image,3))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distort a single image using TensorFlow Image\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph) as session:\n",
    "# Input data, with a placeholder to feed the training data at run time.\n",
    "    input_image = tf.placeholder(tf.float32, shape=training_image.shape)\n",
    "    output = tf.image.random_contrast(input_image, 0.3, 1.8, seed=None)\n",
    "    transposed = session.run([output], feed_dict={input_image:training_image})\n",
    "\n",
    "data = np.asarray(transposed)\n",
    "figure = plt.figure(figsize=(1,1))\n",
    "plt.imshow(np.rot90(data[0],3))   \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomly modify contrast of all the testing images\n",
    "testing_dataset = testing_tuple[0]\n",
    "modified_testing_dataset = testing_dataset.copy()\n",
    "\n",
    "graph = tf.Graph()\n",
    "#Build computaion graph\n",
    "with tf.Session(graph=graph) as session:\n",
    "    input_image = tf.placeholder(tf.float32, shape=training_image.shape)\n",
    "    output = tf.image.random_contrast(input_image, 0.3, 1.8, seed=None)\n",
    "    for i,j in enumerate(modified_testing_dataset):\n",
    "        output_image = session.run([output], feed_dict={input_image:j.reshape(32,32,3)})\n",
    "        modified_testing_dataset[i] = np.asarray(output_image).reshape(3072)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) Repeat the baseline run analysis in 3) on the distorted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is taken from Section 7 notebook\n",
    "#Some utility functions to be used for computation graph building and execution\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "split_by_half = lambda x,k : int(x/2**k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save training, testing and validation datasets \n",
    "#in corresponding variables\n",
    "training_data = training_tuple[0]\n",
    "testing_data = modified_testing_dataset\n",
    "validation_data = validation_tuple[0]\n",
    "#Constants\n",
    "hidden_nodes = 2000\n",
    "batch_size = 256\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "#Dictionary to save prediction results\n",
    "test_preds={}\n",
    "#Regularization penalty\n",
    "regularization_constant = 0.01\n",
    "#The following code is adapted from Section 7 notebook\n",
    "#This portion will construct the computation Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_of_pixels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    \n",
    "    #Constants\n",
    "    tf_valid_dataset = tf.constant(validation_tuple[0])\n",
    "    tf_test_dataset = tf.constant(modified_testing_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([num_of_pixels, hidden_nodes])\n",
    "    layer1_biases = bias_variable([hidden_nodes])\n",
    "    layer2_weights = weight_variable([hidden_nodes, split_by_half(hidden_nodes,1)])\n",
    "    layer2_biases = bias_variable([split_by_half(hidden_nodes,1)])   \n",
    "    layer3_weights = weight_variable([split_by_half(hidden_nodes,1), 10])\n",
    "    layer3_biases = bias_variable([10])\n",
    "    \n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    \n",
    "    # Model with dropout\n",
    "    def model(data):\n",
    "        layer1 = tf.matmul(data, layer1_weights) + layer1_biases\n",
    "        hidden1 = tf.nn.relu(layer1)  \n",
    "        layer2 = tf.matmul(hidden1, layer2_weights) + layer2_biases  # a new hidden layer\n",
    "        hidden2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        return tf.matmul(hidden2, layer3_weights) + layer3_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \\\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + \\\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    loss = tf.reduce_mean(loss + regularization_constant * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    # learning rate decay\n",
    "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from Section 7 notebook\n",
    "def run_session(num_epochs, name):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q5Logs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch accuracy: {:.1f} %\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f} %\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f} %\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()\n",
    "\n",
    "run_session(2000, \"Q5A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c) Comment on what you learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Testing accuracy has dropped from 42.0% to 39.6% when the testing data had random contrast adjustment. This was somewhat expected as our baseline model was not trained with images with wide dynamic range. For a good model it is often recommended to randomly flip images, take transpose, change brightness, change contrast of the original images and feed them to Neural Net in the training phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Compare the tuning of hyperparameters across architectural models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had divided the homework and Q6 is done in a separate notebook submitted with this homework. We apologise for any inconvenience. Our systems had issues. Please check in the separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7) Maximize your performance with your deep neural network or the CNN \n",
    "  1. Tune the parameters\n",
    "  2. Discover what works best and walk us through what you tried\n",
    "  3. Submit the performance as a %accuracy and as a 10x10 table in the format showing the counts of the test images actual vs. predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a) Tune the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline Convolutional Neural Network in 3b gave a testing accuracy of 36.5% which is not acceptable ofcourse. The first question that occurs to one's mind is that if convolutional networks are state of the art why are we getting such a terrible accuracy. \n",
    "\n",
    "The response to this question is simple. In Q3c our focus was not on optimization. It was a brute force application of CNN without optimizing the parameters.\n",
    "\n",
    "Now lets think of what parameters can one change.\n",
    "\n",
    "1. Number of Convolution and MaxPooling Layers \n",
    "2. Try Avergage Pooling instead of MaxPooling\n",
    "3. Add Local Contrast Normalization Layer\n",
    "4. Change Strides\n",
    "5. Change Padding type\n",
    "6. Change dropout probability \n",
    "7. Change number of hidden nodes in the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach that we will use in this question will be to modify some of these parameters and assess its impact on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code is taken from Section 7\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-define the function to include the keep probability\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        merged = tf.merge_all_summaries()  \n",
    "        writer = tf.train.SummaryWriter(\"/tmp/HW4Q7logs\", session.graph)\n",
    "        tf.initialize_all_variables().run()\n",
    "        print(\"Initialized\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (training_labels_enc.shape[0] - batch_size)\n",
    "            batch_data = training_data[offset:(offset + batch_size), :]\n",
    "            batch_data = batch_data.reshape(-1,32,32,3)\n",
    "            batch_labels = training_labels_enc[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), validation_labels_enc)))\n",
    "        model_time = time.time() - start_time\n",
    "        print(\"Time Taken for Training: {:.1f} seconds\".format(model_time))\n",
    "        print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), testing_labels_enc)))\n",
    "        test_preds[name] = test_prediction.eval().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Change the dropout probability (Drop slightly more number of nodes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_session(2000,\"CNN\", 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Increase batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_session(2000,\"CNN\", 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in batch size increased the training time but also increased the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Let us try L2 norm Regularization instead of dropouts to observe the impact**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "#Regularization penalty\n",
    "regularization_constant = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \\\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + \\\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) + \\\n",
    "                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss( layer4_biases))\n",
    "    \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + regularization_constant *regularizers)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Optimizer.\n",
    "    #optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    # learning rate decay\n",
    "    global_step = tf.Variable(0)  # count  number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_session(2000,\"CNNRegularization\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly changing the dropout to L2 did not improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)Let us increase the number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is adapted from the code given in Section 7\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "number_of_channels = 3\n",
    "num_of_pixels = image_width*image_height*number_of_channels\n",
    "num_labels = 10\n",
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, number_of_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    validation_data = validation_tuple[0].reshape(-1,32,32,3)\n",
    "    testing_data = testing_tuple[0].reshape(-1,32,32,3)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, number_of_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2])\n",
    "    layer2_biases = bias_variable([depth2])\n",
    "    layer3_weights = weight_variable([image_width // 4 * image_width // 4 * depth2, num_hidden])\n",
    "    layer3_biases = bias_variable([num_hidden])\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels])\n",
    "    layer4_biases = bias_variable([num_labels])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Model with dropout\n",
    "    def model(data, proba=keep_prob):\n",
    "        # Convolution\n",
    "        conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases\n",
    "        pooled1 = tf.nn.max_pool(tf.nn.relu(conv1), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Convolution\n",
    "        conv2 = tf.nn.conv2d(pooled1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases\n",
    "        pooled2 = tf.nn.max_pool(tf.nn.relu(conv2), ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # Fully Connected Layer\n",
    "        shape = pooled2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooled2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        full3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        # Dropout\n",
    "        full3 = tf.nn.dropout(full3, proba)\n",
    "        return tf.matmul(full3, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    #Reshape tf_train_dataset\n",
    "    logits = model(tf_train_dataset, keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_session(20000,\"CNNMoreEpochs\", 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is considerable improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b) Discover what works best and walk us through what you tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy was to change one parameter at a time and avoid running models that take too long to run as our machines were not very powerful and we did not have enough time to spin up machines in AWS.\n",
    "\n",
    "We therefore started with changing the dropout probability from 0.5 to 0.65 to randomly remove output from greater number of neurons. This slightly improved the accuracy. We also tried setting dropout to a very small value but that reduced the testing accuracy.\n",
    "\n",
    "We then changed the batch size that significantly improved the accuracy from 36.1% to 49.8%. This was a significant jump. In our view the reason that it worked was because at every epoch we were showing more training samples with an increased probability of encountering examples of different classes in a single batch then that of a small batch.\n",
    "\n",
    "The next thing we wanted to try was to replace Drop-out with another regularization method and used L2 regularizer but that decreased the accuracy.\n",
    "\n",
    "In the next step we gradualy increased the number of epochs and the biggest number of epochs we used were 20,000. This took almost 4 hours but improved the accuracy by almost 20%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7c) Submit the performance as a %accuracy and as a 10x10 table in the format showing the counts of the test images actual vs. predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the confusion matrix should have been for the best model but as our best model takes 4 hours and we have run out of time, we are using our second best model to compute confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model Accuracy: CNN 49.3% **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to decode one-hot encoded data and\n",
    "# convert into label\n",
    "def label_decoder(one_hot_encoded):\n",
    "    labels = np.zeros(one_hot_encoded.shape[0])\n",
    "    for index in range(one_hot_encoded.shape[0]):\n",
    "        labels[index]=list(one_hot_encoded[index]).index(1)\n",
    "    return labels\n",
    "    \n",
    "#Method to convert prediction probabilities(i.e softmax output to label)\n",
    "def softmax_to_label(softmaxed):\n",
    "    labels = np.zeros(softmaxed.shape[0])\n",
    "    for index in range(softmaxed.shape[0]):\n",
    "        labels[index] = np.argmax(softmaxed[index])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since we used ravel after running tf model, we need to reshape the predictions\n",
    "testing_labels = label_decoder(testing_labels_enc)\n",
    "predictions = test_preds['CNN']\n",
    "predictions = predictions.reshape(testing_labels_enc.shape[0],testing_labels_enc.shape[1])\n",
    "prediction_labels = softmax_to_label(predictions)\n",
    "\n",
    "confusion_matrix(testing_labels,prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix shall be submitted as in Microsoft Excel format with Predicted, Actual headings etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Exploratory Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried using skflow which provides Scikit-Learn kind of format in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model using 2 convolution layers, 2 maxpool layers and dropout as regularization\n",
    "def model_skflow(X,y):\n",
    "    with tf.variable_scope('conv_1'):\n",
    "        conv1 = layers.convolution2d(X, num_outputs=32, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    with tf.variable_scope('conv_2'):\n",
    "        conv2 = layers.convolution2d(pool1, num_outputs=64, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n",
    "        pool2 = tf.nn.max_pool(pool1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #print(pool2.get_shape())\n",
    "        #This displayed it as (?,8,8, 32)\n",
    "        pool_flattened = tf.reshape(pool2,[-1,8*8*32])\n",
    "    #Dropout for regularization\n",
    "    final = tf.contrib.layers.dropout(tf.contrib.layers.legacy_fully_connected(pool_flattened, 1024, weight_init=None, activation_fn=tf.nn.relu))\n",
    "    return learn.models.logistic_regression(final, y)\n",
    "#Define the model\n",
    "\n",
    "classifier = learn.TensorFlowEstimator(model_fn=model_skflow, n_classes=10, batch_size=100, steps=10,learning_rate=0.001)\n",
    "classifier.fit(training_images_f,label_decoder(training_labels_encf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(label_decoder(testing_labels_encf), classifier.predict(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is really low but the idea was to use a new option available to run TensorFlow"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
